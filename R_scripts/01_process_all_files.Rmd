---
title: "National SET Outputs"
author: "Kim Cressman"
date: "`r Sys.Date()`"
output: 
        html_document:
                toc: yes
---
2/12/2020 - I've received GPS coordinates for some SETs that didn't have them before (the NARUNH ones). Rerunning everything with these included.  

2/11/2020 - have updated all data files; they should be all set. About to run this with everything being up-to-date. have also replaced the sourced "000_functions.R" file with the most up-to-date one. Also made some updates in processing script due to tidyr updates. Everything worked running step by step, so I'm going to knit this and keep the record. But data should be all set now.    

6/26/19 - Have updated site names in a few data/metadata files, so am rerunning this.
-- Have also inserted code to wrangle and combine metadata files.  

7/14/19 - another update, for metadata only:  

+  pulling out NAVD88 Ground and Receiver elevation. There is some confusion about what each one represents but it can still be useful as a y-axis to see relative change related to elevation. 9 reserves have NAVD88 receiver elevations and 6 have ground. 2 of the ones that have ground elevation have *only* that, and not receiver elevation - so 11 reserves (12 once I insert GND's into our metadata) have *something*.  
+  PDB has different "user-friendly SET names"  


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, message = FALSE, warning = FALSE}

library(dplyr)
library(tidyr)
library(purrr)
library(readxl)
library(stringr)
library(lubridate)
library(here)
library(nlme)
library(janitor)
```


# Tidy the individual raw data files  

Find and list the raw files.  

```{r}
raw_path <- here::here("data", "raw_reserve_level")
raw_files <- dir(raw_path, pattern = "set.xls")
```


Process them and output them to the `processed_reserve_level` directory.  

```{r}
processed_path <- here::here("data", "processed_reserve_level")
sourced_path <- here::here("R_scripts", "sourced")
processing_script <- paste0(sourced_path, "/01_natl_process_raw_data.R")

# for each raw file, source the processing script
for(i in seq_along(raw_files)){
        file_path <- paste0(raw_path, "/", raw_files[i])
        source(processing_script)
        message(paste("\n", file_out, "\nhas been produced \n"))
}

message("\n \nFinished with file processing \n")
```


# Combine the individual tidy files into one  


Now read them all in and combine them into one big file.  

```{r}
processed_files <- dir(processed_path, pattern = "processed.csv")

processed_list <- list()

for(i in seq_along(processed_files)){
        # read in file
        file_path <- paste0(processed_path, "/", processed_files[i])
        dat <- read.csv(file_path, stringsAsFactors = FALSE)
        
        # if there isn't a column for reserve, pull it from the file name
        if(!exists("reserve", dat)){
                reserve <- substr(processed_files[i], 1, 3)
                dat$reserve <- toupper(reserve)
        }
        
        # select only the common columns for this project
        dat <- dat %>% 
                mutate_at(c("reserve", "set_id", "arm_position"), as.character) %>% 
                select(reserve,
                       set_id, 
                       year, month, day, 
                       arm_position, 
                       arm_qaqc_code, 
                       pin_number, 
                       starts_with("height"),    # get either mm or cm
                       qaqc_code)
        processed_list[[i]] <- dat
}
```


See the names in all the files.  

```{r}
purrr::map(processed_list, ~names(.x))
```

Bind them into one data frame.  

```{r}
dat_all <- bind_rows(processed_list)

glimpse(dat_all)

unique(dat_all$set_id)
```


Get all of the pin heights into mm and get rid of anything where date doesn't exist.

```{r}
# this only works because we have some of each
# it threw an error when I was testing with only files that have height_mm
dat_all2 <- dat_all %>% 
        mutate(pin_height = case_when(!is.na(height_mm) ~ height_mm,
                                      !is.na(height_cm) ~ 10 * height_cm),
               date = ymd(paste(year, month, day, sep = "\\/")),
               date2 = decimal_date(date),
               arm_pin = paste0(arm_position, "-", pin_number)) %>% 
        select(-starts_with("height")) %>% 
        filter(!is.na(date))

# some of these failed to parse; check out what's going on
failed_parsing <- anti_join(dat_all, dat_all2)
glimpse(failed_parsing)

# looks like a bunch of missing measurements - because there are no dates
# make sure
sum(is.na(dat_all$year))
sum(is.na(dat_all$year)) == nrow(failed_parsing)
# okay
```

Write out to a csv for later use.  

```{r}
out_path <- here::here("data", "processed_combined", "all_sites.csv")
write.csv(dat_all2, out_path, row.names = FALSE)
```


# Read in and combine metadata files  

Find and list the raw files.  

```{r}
raw_path <- here::here("metadata", "reserve_level")
raw_files <- dir(raw_path, pattern = "set_metadata.xls")
```

Read them in and combine them.  

```{r}
combined_list <- list()

for(i in seq_along(raw_files)){
        in_path <- here::here("metadata", "reserve_level", raw_files[i])
        dat <- read_excel(in_path) %>% 
                clean_names() %>% 
                select(reserve,
                       set_id = unique_set_id,
                       user_friendly_set_name,
                       numerical_order,
                       set_type,
                       lat = latitude_dec_deg,
                       long = longitude_dec_deg,
                       navd88_ground = surface_elevation_ground_navd88,
                       navd88_receiver = surface_elevation_receiver_navd88,
                       starts_with("co_dominant_species"),
                       general_salinity) %>% 
                mutate_at(c("reserve", "set_id", "user_friendly_set_name", "general_salinity", "navd88_ground", "navd88_receiver"), as.character) 
        combined_list[[i]] <- dat
}

metadat_all <- bind_rows(combined_list)
```

Write that out.  

```{r}
out_path <- here::here("metadata", "combined", "all_sites_metadata.csv")
write.csv(metadat_all, out_path, row.names = FALSE)
```

